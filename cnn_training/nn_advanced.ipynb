{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataLoader and Dataset\n",
    "\n",
    "We are using the fashion mnist dataset.\n",
    "https://pytorch.org/docs/master/torchvision/datasets.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cpu')\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                     std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, \n",
    "                          train=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "#flatten x_train 60000x28x28\n",
    "x_train = trainset.train_data.view(trainset.train_data.shape[0], -1).float()\n",
    "y_train = trainset.train_labels\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12a1a1a58>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "# each row of x_train conataines a flattend images, we need to reshape it to 28x28 \n",
    "pyplot.imshow(x_train[49948].reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `Dataloader` to generate and iterate over batches. A `Dataloader` can be created from any `Dataset`, with an additional parameter `batch_size` that defines the size of the minibatches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "train_batches = iter(trainloader)\n",
    "images, labels = train_batches.next()\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn.functional\n",
    "Pytroch provides several activation and loss functions in the `torch.nn.functional` module, By convention, this is imported into the namspace `F`.\n",
    "\n",
    "Since we are using negative log likelihood loss and log softmax activation, then Pytorch provides a single function F.cross_entropy that combines the two. So we can even remove the activation function from our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb): return xb @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(138.8269, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "weights = torch.randn(784,10)/math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)\n",
    "loss_func(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using torch.nn.Module\n",
    "We'll use nn.Module and nn.Parameter, for a clearer and more concise training loop. We subclass nn.Module (which itself is a class and able to keep track of state). In this case, we want to create a class that holds our weights, bias, and method for the forward step. nn.Module has a number of attributes and methods (such as `.parameters()` and `.zero_grad()`) which we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Logistic_mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784,10)/math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb): return xb @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(115.1345, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Logistic_mnist()\n",
    "loss_func(model(x_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simplify training using `Dataloader` and `Logistic_minist` model. Here we will define a function `fit()` for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader, epochs, lr):\n",
    "    for epoch in range(epochs):\n",
    "        for x_train, y_train in dataloader:\n",
    "            x_train = x_train.view(x_train.shape[0], -1)\n",
    "            preds = model(x_train)\n",
    "            loss = loss_func(preds, y_train)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5   # learning rate\n",
    "epochs = 3 # how many epochs to train for\n",
    "model = Logistic_mnist()\n",
    "dataloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "fit(model, dataloader, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using nn.Linear\n",
    "Using predefined `Linear` for the linear layer, simplifies model definition and `fit()` will run faster. We will also flatten the input for the linear layer in the `forward()` method.\n",
    "So the input should have size `64x1x28x28` which will be handy when we add a convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_mnist(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(784,10)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        #flatten x before linear layer\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "def fit(model, dataloader, epochs, lr):\n",
    "    for epoch in range(epochs):\n",
    "        for x_train, y_train in dataloader:\n",
    "            preds = model(x_train)\n",
    "            loss = loss_func(preds, y_train)\n",
    "            \n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters(): p -= p.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5   # learning rate\n",
    "epochs = 3 # how many epochs to train for\n",
    "model = Logistic_mnist()\n",
    "\n",
    "fit(model, dataloader, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using optim\n",
    "\n",
    "Pytorch also has a package with various optimization algorithms, torch.optim. We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "def get_model(lr):\n",
    "    model = Logistic_mnist()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def fit(model, opt, dataloader, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for x_train, y_train in dataloader:\n",
    "            preds = model(x_train)\n",
    "            loss = loss_func(preds, y_train)\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 3\n",
    "model, opt = get_model(lr)\n",
    "fit(model, opt, dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "It is important to use separate data sets for training and validation. Doing so, we will be able to determine if we are overfitting. Also, shuffling of the training data is important to reduce the correlation between batches, wheras this is not necessary for the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                     std=[0.229, 0.224, 0.225])])\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, \n",
    "                          train=True, transform=transform)\n",
    "\n",
    "train_dl = DataLoader(trainset, batch_size=64)\n",
    "validationset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, \n",
    "                          train=False, transform=transform)\n",
    "validation_dl = DataLoader(validationset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware of using `model.train()` and `model.eval()` in the `fit()` function!\n",
    "\n",
    "By default all the modules are initialized to train mode (self.training = True). Some layers might have different behavior during train/and evaluation (like BatchNorm, Dropout) so setting it matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.357490782928467\n",
      "1 5.42783166809082\n",
      "2 5.9633808494567875\n",
      "3 5.46928278503418\n",
      "4 6.572009448242188\n",
      "5 5.078942433929443\n",
      "6 4.712965372085571\n",
      "7 6.043066973876953\n",
      "8 5.022225035858154\n",
      "9 5.165457396316528\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "def get_model(lr):\n",
    "    model = Logistic_mnist()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "def calculate_loss(model, loss_func, x_train, y_train, opt=None):\n",
    "    loss = loss_func(model(x_train), y_train)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(x_train)\n",
    "\n",
    "def fit(model, opt, loss_func, train_dl, validation_dl, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x_train, y_train in train_dl:\n",
    "            calculate_loss(model, loss_func, x_train, y_train, opt)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums = \\\n",
    "            zip(*[calculate_loss(model, loss_func, x, y) for x, y in validation_dl])\n",
    "        val_loss = np.sum(np.multiply(losses,nums)) / np.sum(nums)\n",
    "        \n",
    "        print(epoch, val_loss)\n",
    "    \n",
    "loss_func = F.cross_entropy\n",
    "lr = 0.5\n",
    "epochs = 10\n",
    "model, opt = get_model(lr)\n",
    "fit(model, opt, loss_func, train_dl, validation_dl, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 4)\n",
      "(2, 10, 5)\n",
      "(3, 100, 6)\n",
      "<zip object at 0x131b8d788>\n",
      "(1, 2, 3)\n",
      "(10, 20, 30)\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [1,10,100]\n",
    "c = [4,5,6,7]\n",
    "r = zip(a,b,c)\n",
    "for i in r:\n",
    "    print(i)\n",
    "    \n",
    "vls = [(1,10), (2,20), (3,30)]\n",
    "rs =zip(*vls)\n",
    "rs = zip((1,10), (2,20), (3,30))\n",
    "print(rs)\n",
    "for j in rs:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
